{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xan0fzM8FvUe"
      },
      "source": [
        "# Diffusion models\n",
        "\n",
        "MNIST data\n",
        "\n",
        "2024-2-10\n",
        "\n",
        "Author: Yung-Kyun Noh, Ph.D.\n",
        "\n",
        "Hanyang University / Korea Institute for Advanced Study\n",
        "\n",
        "** BIML 2024 **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IASEUwBFvUk"
      },
      "source": [
        "This notebook implements simple diffusion models using MNIST set.\n",
        "\n",
        "The codes are based on the functions modified from the following example,\n",
        "\n",
        "https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing,\n",
        "\n",
        "which has been explained in\n",
        "\n",
        "https://medium.com/mlearning-ai/enerating-images-with-ddpms-a-pytorch-implementation-cef5a2ba8cb1.\n",
        "\n",
        "The implementation of the second UNet is from the hands-on codes used in the following NVIDIA DLI program:\n",
        "\n",
        "https://www.nvidia.com/en-us/training/instructor-led-workshops/generative-ai-with-diffusion-models/.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4hfF9UGeANL"
      },
      "outputs": [],
      "source": [
        "# Import of libraries\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import einops\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda\n",
        "from torchvision.datasets.mnist import MNIST, FashionMNIST\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "# Setting reproducibility\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhyEQtM67YVJ"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnQA-V2TEd6d"
      },
      "outputs": [],
      "source": [
        "def show_images(images, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "\n",
        "    # Converting images to CPU numpy arrays\n",
        "    if type(images) is torch.Tensor:\n",
        "        images = images.detach().cpu().numpy()\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    rows = int(len(images) ** (1 / 2))\n",
        "    cols = round(len(images) / rows)\n",
        "\n",
        "    print('Number of images:', len(images))\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if idx < len(images):\n",
        "                fig.add_subplot(rows, cols, idx + 1)\n",
        "                plt.imshow(images[idx][0], cmap=\"gray\")\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "\n",
        "    # Showing the figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkWnTOS0EitS"
      },
      "outputs": [],
      "source": [
        "def show_first_batch(loader):\n",
        "    for batch in loader:\n",
        "        show_images(batch[0], \"Images in the first batch\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4FlAq63FvUp"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwEYqhOGFvUq"
      },
      "outputs": [],
      "source": [
        "img_size = 28\n",
        "batch_size = 128\n",
        "\n",
        "data_transforms = [\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),  # Scales data into [0,1]\n",
        "]\n",
        "\n",
        "data_transform = transforms.Compose(data_transforms)\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    \"./data/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=data_transform,\n",
        "    )\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    \"./data/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=data_transform,\n",
        "    )\n",
        "data = torch.utils.data.ConcatDataset([train_set, test_set])\n",
        "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOFIp14opS4K"
      },
      "outputs": [],
      "source": [
        "# Optionally, show a batch of regular images\n",
        "show_first_batch(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXPKke_S76cb"
      },
      "outputs": [],
      "source": [
        "# Getting device\n",
        "run_gpu = 1    # 0,1,2,3,...\n",
        "dev = 'cuda:' + str(run_gpu)\n",
        "# dev='cpu'\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device(dev if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\t\" + (f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY_qhk428P17"
      },
      "source": [
        "# DDPM module\n",
        "\n",
        "\n",
        "Backbone DDPM module based on the codes in\n",
        "\n",
        "https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing,\n",
        "\n",
        "\n",
        "- `n_steps`: number of diffusion steps $T$;\n",
        "- `min_beta`: value of the first $\\beta_t$ ($\\beta_1$);\n",
        "- `max_beta`: value of the last  $\\beta_t$ ($\\beta_T$);\n",
        "- `device`: device onto which the model is run;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Menl_lR8SCC"
      },
      "outputs": [],
      "source": [
        "# DDPM class\n",
        "class MyDDPM(nn.Module):\n",
        "    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None):\n",
        "        super(MyDDPM, self).__init__()\n",
        "        self.n_steps = n_steps\n",
        "        self.device = device\n",
        "        self.network = network.to(device)\n",
        "        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(\n",
        "            device)  # Number of steps is typically in the order of thousands\n",
        "        self.alphas = 1 - self.betas\n",
        "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)\n",
        "\n",
        "    def forward(self, x0, t, eta=None):\n",
        "        # Make input image more noisy (we can directly skip to the desired step)\n",
        "        n, c, h, w = x0.shape\n",
        "        a_bar = self.alpha_bars[t]\n",
        "\n",
        "        if eta is None:\n",
        "            eta = torch.randn(n, c, h, w).to(self.device)\n",
        "\n",
        "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta\n",
        "        return noisy\n",
        "\n",
        "    def backward(self, x, t):\n",
        "        # Run each image through the network for each timestep t in the vector t.\n",
        "        # The network returns its estimation of the noise that was added.\n",
        "        return self.network(x, t)\n",
        "\n",
        "    def backward_cfg(self, x, t, c, c_mask):   # Classifier-free guidance\n",
        "        return self.network(x, t, c, c_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbDZPtutErS8"
      },
      "source": [
        "## Visualizing forward and backward\n",
        "\n",
        "Among the two options in the paper, (https://arxiv.org/pdf/2006.11239.pdf) by Ho et. al.,\n",
        "\n",
        "- $\\sigma_t^2$ = $\\beta_t$\n",
        "- $\\sigma_t^2$ = $\\frac{1 - \\bar{\\alpha_{t-1}}}{1 - \\bar{\\alpha_{t}}} \\beta_t$,\n",
        "\n",
        "The first option $\\sigma_t^2$ = $\\beta_t$ is chosen in this implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY416VfcErbO"
      },
      "outputs": [],
      "source": [
        "def show_forward(ddpm, loader, device):\n",
        "    # Showing the forward process\n",
        "    for batch in loader:\n",
        "        imgs = batch[0]\n",
        "\n",
        "        show_images(imgs, \"Original images\")\n",
        "\n",
        "        for percent in [0.25, 0.5, 0.75, 1]:\n",
        "            show_images(\n",
        "                ddpm(imgs.to(device),\n",
        "                     [int(percent * ddpm.n_steps) - 1 for _ in range(len(imgs))]),\n",
        "                f\"DDPM Noisy images {int(percent * 100)}%\"\n",
        "            )\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQGApj4_Ewmt"
      },
      "outputs": [],
      "source": [
        "def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name=\"sampling.gif\", c=1, h=28, w=28):\n",
        "    \"\"\"Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples\"\"\"\n",
        "    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)\n",
        "    frames = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if device is None:\n",
        "            device = ddpm.device\n",
        "\n",
        "        # Starting from random noise\n",
        "        x = torch.randn(n_samples, c, h, w).to(device)\n",
        "\n",
        "        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):\n",
        "            # Estimating noise to be removed\n",
        "            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()\n",
        "            eta_theta = ddpm.backward(x, time_tensor)\n",
        "\n",
        "            alpha_t = ddpm.alphas[t]\n",
        "            alpha_t_bar = ddpm.alpha_bars[t]\n",
        "\n",
        "            # Partially denoising the image\n",
        "            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)\n",
        "\n",
        "            if t > 0:\n",
        "                z = torch.randn(n_samples, c, h, w).to(device)\n",
        "\n",
        "                # Option 1: sigma_t squared = beta_t\n",
        "                beta_t = ddpm.betas[t]\n",
        "                sigma_t = beta_t.sqrt()\n",
        "\n",
        "                # Option 2: sigma_t squared = beta_tilda_t\n",
        "                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t > 0 else ddpm.alphas[0]\n",
        "                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t\n",
        "                # sigma_t = beta_tilda_t.sqrt()\n",
        "\n",
        "                # Adding some more noise like in Langevin Dynamics fashion\n",
        "                x = x + sigma_t * z\n",
        "\n",
        "            # Adding frames to the GIF\n",
        "            if idx in frame_idxs or t == 0:\n",
        "                # Putting digits in range [0, 255]\n",
        "                normalized = x.clone()\n",
        "                for i in range(len(normalized)):\n",
        "                    normalized[i] -= torch.min(normalized[i])\n",
        "                    normalized[i] *= 255 / torch.max(normalized[i])\n",
        "\n",
        "                # Reshaping batch (n, c, h, w) to be a (as much as it gets) square frame\n",
        "                frame = einops.rearrange(normalized, \"(b1 b2) c h w -> (b1 h) (b2 w) c\", b1=int(n_samples ** 0.5))\n",
        "                frame = frame.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "                # Rendering frame\n",
        "                frames.append(frame)\n",
        "\n",
        "    # Storing the gif\n",
        "    with imageio.get_writer(gif_name, mode=\"I\") as writer:\n",
        "        for idx, frame in enumerate(frames):\n",
        "            rgb_frame = np.repeat(frame, 3, axis=2)\n",
        "            writer.append_data(rgb_frame)\n",
        "\n",
        "            # Showing the last frame for a longer time\n",
        "            if idx == len(frames) - 1:\n",
        "                last_rgb_frame = np.repeat(frames[-1], 3, axis=2)\n",
        "                for _ in range(frames_per_gif // 3):\n",
        "                    writer.append_data(last_rgb_frame)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GDfFXcGDe3D"
      },
      "source": [
        "# UNet architecture\n",
        "\n",
        "A vanilla UNet structure was implemented from\n",
        "\n",
        "https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing,\n",
        "\n",
        "For 28x28 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqvzCL5eDnSf"
      },
      "outputs": [],
      "source": [
        "class MyUNet(nn.Module):\n",
        "    def __init__(self, n_steps=1000, time_emb_dim=100):\n",
        "        super(MyUNet, self).__init__()\n",
        "\n",
        "        # Sinusoidal embedding\n",
        "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "        # First half\n",
        "        self.te1 = self._make_te(time_emb_dim, 1)\n",
        "        self.b1 = nn.Sequential(\n",
        "            MyBlock((1, 28, 28), 1, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10)\n",
        "        )\n",
        "        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
        "\n",
        "        self.te2 = self._make_te(time_emb_dim, 10)\n",
        "        self.b2 = nn.Sequential(\n",
        "            MyBlock((10, 14, 14), 10, 20),\n",
        "            MyBlock((20, 14, 14), 20, 20),\n",
        "            MyBlock((20, 14, 14), 20, 20)\n",
        "        )\n",
        "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
        "\n",
        "        self.te3 = self._make_te(time_emb_dim, 20)\n",
        "        self.b3 = nn.Sequential(\n",
        "            MyBlock((20, 7, 7), 20, 40),\n",
        "            MyBlock((40, 7, 7), 40, 40),\n",
        "            MyBlock((40, 7, 7), 40, 40)\n",
        "        )\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(40, 40, 2, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(40, 40, 4, 2, 1)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.te_mid = self._make_te(time_emb_dim, 40)\n",
        "        self.b_mid = nn.Sequential(\n",
        "            MyBlock((40, 3, 3), 40, 20),\n",
        "            MyBlock((20, 3, 3), 20, 20),\n",
        "            MyBlock((20, 3, 3), 20, 40)\n",
        "        )\n",
        "\n",
        "        # Second half\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(40, 40, 4, 2, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
        "        )\n",
        "\n",
        "        self.te4 = self._make_te(time_emb_dim, 80)\n",
        "        self.b4 = nn.Sequential(\n",
        "            MyBlock((80, 7, 7), 80, 40),\n",
        "            MyBlock((40, 7, 7), 40, 20),\n",
        "            MyBlock((20, 7, 7), 20, 20)\n",
        "        )\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
        "        self.te5 = self._make_te(time_emb_dim, 40)\n",
        "        self.b5 = nn.Sequential(\n",
        "            MyBlock((40, 14, 14), 40, 20),\n",
        "            MyBlock((20, 14, 14), 20, 10),\n",
        "            MyBlock((10, 14, 14), 10, 10)\n",
        "        )\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
        "        self.te_out = self._make_te(time_emb_dim, 20)\n",
        "        self.b_out = nn.Sequential(\n",
        "            MyBlock((20, 28, 28), 20, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10, normalize=False)\n",
        "        )\n",
        "\n",
        "        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)\n",
        "        t = self.time_embed(t)\n",
        "        n = len(x)\n",
        "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)\n",
        "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)\n",
        "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)\n",
        "\n",
        "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)\n",
        "\n",
        "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)\n",
        "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)\n",
        "\n",
        "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)\n",
        "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)\n",
        "\n",
        "        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)\n",
        "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)\n",
        "\n",
        "        out = self.conv_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _make_te(self, dim_in, dim_out):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(dim_in, dim_out),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim_out, dim_out)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR-K3U6rDiJN"
      },
      "outputs": [],
      "source": [
        "class MyBlock(nn.Module):\n",
        "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
        "        super(MyBlock, self).__init__()\n",
        "        self.ln = nn.LayerNorm(shape)\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
        "        self.activation = nn.SiLU() if activation is None else activation\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ln(x) if self.normalize else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.activation(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y4vi6nCDfDG"
      },
      "outputs": [],
      "source": [
        "def sinusoidal_embedding(n, d):\n",
        "    # Returns the standard positional embedding\n",
        "    embedding = torch.zeros(n, d)\n",
        "    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])\n",
        "    wk = wk.reshape((1, d))\n",
        "    t = torch.arange(n).reshape((n, 1))\n",
        "    embedding[:,::2] = torch.sin(t * wk[:,::2])\n",
        "    embedding[:,1::2] = torch.cos(t * wk[:,::2])\n",
        "\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_CRAXiJFvUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci0Qtref8bff"
      },
      "source": [
        "# Initiating the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8YotKGi8dYW"
      },
      "outputs": [],
      "source": [
        "# Defining model\n",
        "n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors\n",
        "ddpm = MyDDPM(MyUNet(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBUbjFMarUFC"
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in ddpm.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHUEtZlP8is6"
      },
      "source": [
        "# Optional visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a5kTn5N8p9r"
      },
      "source": [
        "Optionally, you can load a pre-trained model that will be further trained using\n",
        "\n",
        "ddpm.load_state_dict(torch.load(store_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1pmuQQP8ixp",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Optionally, show the diffusion (forward) process\n",
        "show_forward(ddpm, dataloader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6nWCYiFvUv"
      },
      "source": [
        "### Optionally, show the denoising (backward) process without"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV6tXQy_8uf1"
      },
      "outputs": [],
      "source": [
        "generated = generate_new_images(ddpm, gif_name=\"before_training.gif\")\n",
        "show_images(generated, \"Images generated before training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjgbi0iRDvH_"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJcKvYEZDvaa"
      },
      "outputs": [],
      "source": [
        "def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path=\"ddpm_model.pt\"):\n",
        "    mse = nn.MSELoss()\n",
        "    best_loss = float(\"inf\")\n",
        "    n_steps = ddpm.n_steps\n",
        "\n",
        "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\", colour=\"#00ff00\"):\n",
        "        epoch_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\")):\n",
        "            # Loading data\n",
        "            x0 = batch[0].to(device)\n",
        "            n = len(x0)\n",
        "\n",
        "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
        "            eta = torch.randn_like(x0).to(device)\n",
        "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
        "\n",
        "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
        "            noisy_imgs = ddpm(x0, t, eta)\n",
        "\n",
        "            # Getting model estimation of noise based on the images and the time-step\n",
        "            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))\n",
        "\n",
        "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
        "            loss = mse(eta_theta, eta)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
        "\n",
        "        # Display images generated at this epoch\n",
        "        if display:\n",
        "            show_images(generate_new_images(ddpm, device=device), f\"Images generated at epoch {epoch + 1}\")\n",
        "\n",
        "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
        "\n",
        "        # Storing the model\n",
        "        if best_loss > epoch_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(ddpm.state_dict(), store_path)\n",
        "            log_string += \" --> Best model ever (stored)\"\n",
        "\n",
        "        print(log_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRTedmnAD24Z"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "store_path = \"ddpm_mnist.pt\"\n",
        "training_loop(ddpm, dataloader, n_epochs, optim=Adam(ddpm.parameters(), lr), device=device, store_path=store_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kdY0ZUuD50F"
      },
      "source": [
        "# Testing the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wGnH4g6D58A"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "best_model = MyDDPM(MyUNet(), n_steps=n_steps, device=device)\n",
        "store_path = \"ddpm_mnist.pt\"\n",
        "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
        "best_model.eval()\n",
        "print(\"Model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r_67l-DEL7y"
      },
      "outputs": [],
      "source": [
        "print(\"Generating new images\")\n",
        "generated = generate_new_images(\n",
        "        best_model,\n",
        "        n_samples=100,\n",
        "        device=device,\n",
        "        gif_name=\"mnist.gif\"\n",
        "    )\n",
        "show_images(generated, \"Final result\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMqv53xOJEyF"
      },
      "source": [
        "# Visualizing the diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnd5RGOyJFKo"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(open('mnist.gif','rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klYJJsaVFvUy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HvX1iIPFvUy"
      },
      "source": [
        "# Use upgraded UNet structure\n",
        "\n",
        "Advanced techniques such as Group normalization and GeLU have been implemented. The new structure is based on the NVIDIA DLI hands on implementation of the UNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkbEPU7LFvUy"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5-C9xg7FvU3"
      },
      "outputs": [],
      "source": [
        "class MyUNet2(nn.Module):\n",
        "    def __init__(self, n_steps=1000, time_emb_dim=8):\n",
        "        super(MyUNet2, self).__init__()\n",
        "\n",
        "        img_size = 28\n",
        "        img_ch = 1\n",
        "        self.T = n_steps\n",
        "        down_chs=(64, 64, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = img_size // 4  # 2 ** (len(down_chs) - 1)\n",
        "        t_embed_dim = time_emb_dim\n",
        "        small_group_size = 8\n",
        "        big_group_size = 32\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = ResidualConvBlock(img_ch, down_chs[0], small_group_size)\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size)\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size)\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2] * latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2] * latent_image_size**2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
        "        self.t_emb1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
        "        self.t_emb2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size),\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size)\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size)\n",
        "\n",
        "        # Match output channels and one last concatenation\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
        "            nn.GroupNorm(small_group_size, up_chs[-1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], img_ch, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        down0 = self.down0(x)\n",
        "        down1 = self.down1(down0)\n",
        "        down2 = self.down2(down1)\n",
        "        latent_vec = self.to_vec(down2)\n",
        "\n",
        "        latent_vec = self.dense_emb(latent_vec)\n",
        "        t = t.float() / self.T  # Convert from [0, T] to [0, 1]\n",
        "        t = self.sinusoidaltime(t)\n",
        "        t_emb1 = self.t_emb1(t)\n",
        "        t_emb2 = self.t_emb2(t)\n",
        "\n",
        "        up0 = self.up0(latent_vec)\n",
        "        up1 = self.up1(up0 + t_emb1, down2)\n",
        "        up2 = self.up2(up1 + t_emb2, down1)\n",
        "\n",
        "        return self.out(torch.cat((up2, down0), 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9lveNIDFvU4"
      },
      "outputs": [],
      "source": [
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GELUConvBlock(in_chs, out_chs, group_size)\n",
        "        self.conv2 = GELUConvBlock(out_chs, out_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        out = x1 + x2\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hABpuHG_FvU4"
      },
      "outputs": [],
      "source": [
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_ch, out_ch, 3, 1, 1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU(),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxqC9IYzFvU4"
      },
      "outputs": [],
      "source": [
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(DownBlock, self).__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMQs3sgEFvU4"
      },
      "outputs": [],
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(UpBlock, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT--PU_aFvU5"
      },
      "outputs": [],
      "source": [
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)\n",
        "        self.conv = GELUConvBlock(4 * in_chs, in_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsSV65BXFvU5"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUrQQwNRFvU5"
      },
      "outputs": [],
      "source": [
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Unflatten(1, (emb_dim, 1, 1)),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYe3p1loFvU5"
      },
      "source": [
        "# Initiating the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnmmWpLQFvU5"
      },
      "source": [
        "Instantiation of an advanced UNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCKMqhonFvU5"
      },
      "outputs": [],
      "source": [
        "# Defining model\n",
        "n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors\n",
        "# MyUNet2\n",
        "ddpm = MyDDPM(MyUNet2(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ8qsA6ZFvU6"
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in ddpm.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-TQ2JdYFvU6"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKB33cFuFvU6"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "store_path = \"ddpm_mnist2.pt\"\n",
        "training_loop(ddpm, dataloader, n_epochs, optim=Adam(ddpm.parameters(), lr), device=device, store_path=store_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxRXIT7RFvU6"
      },
      "source": [
        "# Testing the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZWLrIb6FvU7"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "best_model = MyDDPM(MyUNet2(), n_steps=n_steps, device=device)\n",
        "store_path = \"ddpm_mnist2.pt\"\n",
        "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
        "best_model.eval()\n",
        "print(\"Model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KQioYDnFvU7"
      },
      "outputs": [],
      "source": [
        "print(\"Generating new images\")\n",
        "\n",
        "generated = generate_new_images(\n",
        "        best_model,\n",
        "        n_samples=100,\n",
        "        device=device,\n",
        "        gif_name=\"mnist2.gif\"\n",
        "    )\n",
        "show_images(generated, \"Final result\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk9DVVVwFvU7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(open('mnist2.gif','rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuOAblmvFvU7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZBEY9wZFvU8"
      },
      "source": [
        "# Classifier-free Guidance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH1yad3-FvU8"
      },
      "source": [
        "The classifier-free guidance model is implemented with the advanced UNet structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OaHYcigFvU8"
      },
      "outputs": [],
      "source": [
        "class MyUNet_CFG(nn.Module):\n",
        "    def __init__(self, n_steps=1000, time_emb_dim=8):\n",
        "        super(MyUNet_CFG, self).__init__()\n",
        "\n",
        "        img_size = 28\n",
        "        img_ch = 1\n",
        "        self.T = n_steps\n",
        "        down_chs=(64, 64, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = img_size // 4  # 2 ** (len(down_chs) - 1)\n",
        "        t_embed_dim = time_emb_dim\n",
        "        c_embed_dim = 10\n",
        "        small_group_size = 8\n",
        "        big_group_size = 32\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = ResidualConvBlock(img_ch, down_chs[0], small_group_size)\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size)\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size)\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2] * latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2] * latent_image_size**2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
        "        self.t_emb1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
        "        self.t_emb2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
        "        self.c_embed1 = EmbedBlock(c_embed_dim, up_chs[0])\n",
        "        self.c_embed2 = EmbedBlock(c_embed_dim, up_chs[1])\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size),\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size)\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size)\n",
        "\n",
        "        # Match output channels and one last concatenation\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
        "            nn.GroupNorm(small_group_size, up_chs[-1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], img_ch, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        down0 = self.down0(x)\n",
        "        down1 = self.down1(down0)\n",
        "        down2 = self.down2(down1)\n",
        "        latent_vec = self.to_vec(down2)\n",
        "\n",
        "        latent_vec = self.dense_emb(latent_vec)\n",
        "        t = t.float() / self.T  # Convert from [0, T] to [0, 1]\n",
        "        t = self.sinusoidaltime(t)\n",
        "        t_emb1 = self.t_emb1(t)\n",
        "        t_emb2 = self.t_emb2(t)\n",
        "\n",
        "        c = c * c_mask\n",
        "        c_emb1 = self.c_embed1(c)\n",
        "        c_emb2 = self.c_embed2(c)\n",
        "\n",
        "        up0 = self.up0(latent_vec)\n",
        "        up1 = self.up1(c_emb1 * up0 + t_emb1, down2)\n",
        "        up2 = self.up2(c_emb2 * up1 + t_emb2, down1)\n",
        "\n",
        "        return self.out(torch.cat((up2, down0), 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1icLcEPFvU8"
      },
      "source": [
        "# Initiate network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z431RfxIFvU9"
      },
      "outputs": [],
      "source": [
        "# Defining model\n",
        "n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors\n",
        "# MyUNet2\n",
        "ddpm = MyDDPM(MyUNet_CFG(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8T_Y1EwFvU9"
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in ddpm.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp6O-TrOFvU9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s__JmONFvU9"
      },
      "outputs": [],
      "source": [
        "def get_context_mask(c, drop_prob, n_classes=10, device='cpu'):\n",
        "    c_hot = F.one_hot(c.to(torch.int64), num_classes=n_classes).to(device)\n",
        "    c_mask = torch.bernoulli(torch.ones_like(c_hot).float() - drop_prob).to(device)\n",
        "    return c_hot, c_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6knkEqARFvU9"
      },
      "outputs": [],
      "source": [
        "def training_loop_cfg(ddpm, loader, n_epochs, optim, device, n_classes=10, c_drop_prob=0.1, display=False, store_path=\"ddpm_model.pt\"):\n",
        "    mse = nn.MSELoss()\n",
        "    best_loss = float(\"inf\")\n",
        "    n_steps = ddpm.n_steps\n",
        "\n",
        "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\", colour=\"#00ff00\"):\n",
        "        epoch_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\")):\n",
        "            # Loading data\n",
        "            x0 = batch[0].to(device)\n",
        "            n = len(x0)\n",
        "            c_hot, c_mask = get_context_mask(batch[1], c_drop_prob, n_classes=n_classes, device=device)  # New\n",
        "\n",
        "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
        "            eta = torch.randn_like(x0).to(device)\n",
        "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
        "\n",
        "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
        "            noisy_imgs = ddpm(x0, t, eta)\n",
        "\n",
        "            # Getting model estimation of noise based on the images and the time-step\n",
        "            eta_theta = ddpm.backward_cfg(noisy_imgs, t.reshape(n, -1), c_hot, c_mask)\n",
        "\n",
        "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
        "            loss = mse(eta_theta, eta)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
        "\n",
        "        # Display images generated at this epoch\n",
        "        if display:\n",
        "            show_images(generate_new_images(ddpm, device=device), f\"Images generated at epoch {epoch + 1}\")\n",
        "\n",
        "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
        "\n",
        "        # Storing the model\n",
        "        if best_loss > epoch_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(ddpm.state_dict(), store_path)\n",
        "            log_string += \" --> Best model ever (stored)\"\n",
        "\n",
        "        print(log_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_1b7cEfFvU-"
      },
      "outputs": [],
      "source": [
        "n_classes = 10\n",
        "\n",
        "# Training\n",
        "store_path = \"ddpm_mnist_cfg.pt\"\n",
        "training_loop_cfg(ddpm, dataloader, n_epochs, optim=Adam(ddpm.parameters(), lr), device=device, \\\n",
        "                  n_classes=n_classes, store_path=store_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4euCAGvFvU-"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "best_model = MyDDPM(MyUNet_CFG(), n_steps=n_steps, device=device)\n",
        "store_path = \"ddpm_mnist_cfg.pt\"\n",
        "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
        "best_model.eval()\n",
        "print(\"Model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yfRCak9FvU-"
      },
      "outputs": [],
      "source": [
        "# with w\n",
        "def generate_new_images_cfg(ddpm, n_classes=10, device=None, frames_per_gif=100, gif_name=\"sampling.gif\", \\\n",
        "                            c=1, h=28, w=28, w_val = 0.):\n",
        "    \"\"\"Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples\"\"\"\n",
        "    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)\n",
        "    frames = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if device is None:\n",
        "            device = ddpm.device\n",
        "\n",
        "        n_samples = n_classes\n",
        "        # Starting from random noise\n",
        "        x = torch.randn(n_samples, c, h, w).to(device)\n",
        "\n",
        "        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):\n",
        "            # Estimating noise to be removed\n",
        "            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()\n",
        "            classes = torch.arange(n_classes).to(device)\n",
        "            c_drop_prob = 0\n",
        "            c_hot, c_mask = get_context_mask(classes, c_drop_prob, device=device)\n",
        "            eta_theta_keep_class = ddpm.backward_cfg(x, time_tensor, c_hot, c_mask)\n",
        "            c_mask = torch.zeros_like(c_mask)\n",
        "            eta_theta_drop_class = ddpm.backward_cfg(x, time_tensor, c_hot, c_mask)\n",
        "            eta_theta = (1 + w_val) * eta_theta_keep_class - w_val * eta_theta_drop_class\n",
        "\n",
        "            alpha_t = ddpm.alphas[t]\n",
        "            alpha_t_bar = ddpm.alpha_bars[t]\n",
        "\n",
        "            # Partially denoising the image\n",
        "            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)\n",
        "\n",
        "            if t > 0:\n",
        "                z = torch.randn(n_samples, c, h, w).to(device)\n",
        "\n",
        "                # Option 1: sigma_t squared = beta_t\n",
        "                beta_t = ddpm.betas[t]\n",
        "                sigma_t = beta_t.sqrt()\n",
        "\n",
        "                # Option 2: sigma_t squared = beta_tilda_t\n",
        "                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t > 0 else ddpm.alphas[0]\n",
        "                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t\n",
        "                # sigma_t = beta_tilda_t.sqrt()\n",
        "\n",
        "                # Adding some more noise like in Langevin Dynamics fashion\n",
        "                x = x + sigma_t * z\n",
        "\n",
        "            # Adding frames to the GIF\n",
        "            if idx in frame_idxs or t == 0:\n",
        "                # Putting digits in range [0, 255]\n",
        "                normalized = x.clone()\n",
        "                for i in range(len(normalized)):\n",
        "                    normalized[i] -= torch.min(normalized[i])\n",
        "                    normalized[i] *= 255 / torch.max(normalized[i])\n",
        "\n",
        "                #  Reshaping batch (n, c, h, w)\n",
        "                frame = einops.rearrange(normalized, \"(b1 b2) c h w -> (b1 h) (b2 w) c\", b1=int(n_samples))\n",
        "                frame = frame.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "                # Rendering frame\n",
        "                frames.append(frame)\n",
        "\n",
        "    # Storing the gif\n",
        "    with imageio.get_writer(gif_name, mode=\"I\") as writer:\n",
        "        for idx, frame in enumerate(frames):\n",
        "            rgb_frame = np.repeat(frame, 3, axis=2)\n",
        "            writer.append_data(rgb_frame)\n",
        "\n",
        "            # Showing the last frame for a longer time\n",
        "            if idx == len(frames) - 1:\n",
        "                last_rgb_frame = np.repeat(frames[-1], 3, axis=2)\n",
        "                for _ in range(frames_per_gif // 3):\n",
        "                    writer.append_data(last_rgb_frame)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNqiFyjBFvU-"
      },
      "outputs": [],
      "source": [
        "def show_images_cfg(images, n_classes, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "\n",
        "    # Converting images to CPU numpy arrays\n",
        "    if type(images) is torch.Tensor:\n",
        "        images = images.detach().cpu().numpy()\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 2))\n",
        "    rows = 1\n",
        "    cols = n_classes\n",
        "\n",
        "    print('Number of images:', len(images))\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if idx < len(images):\n",
        "                fig.add_subplot(rows, cols, idx + 1)\n",
        "                plt.imshow(images[idx][0], cmap=\"gray\")\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "\n",
        "    # Showing the figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFtHWeVVFvU_"
      },
      "source": [
        "### Show the classifier-free guidance generation\n",
        "\n",
        "With $w = 0$, the generation is the pure classifier-free guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "o8T6eeJYFvU_"
      },
      "outputs": [],
      "source": [
        "print(\"Generating new images\")\n",
        "\n",
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\"\n",
        "    )\n",
        "print(generated.shape)\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_oA9PluFvU_"
      },
      "outputs": [],
      "source": [
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\"\n",
        "    )\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMVj8AN4FvVA"
      },
      "source": [
        "With $w = 1$, the noise toward the noise for the class has been emphasized. Image quality should become better with the sacrifice of the diversity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmd57gR5FvVA"
      },
      "outputs": [],
      "source": [
        "# w = 1.\n",
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\",\n",
        "        w_val = 1.\n",
        "    )\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcN61P_sFvVA"
      },
      "outputs": [],
      "source": [
        "# w = 1.\n",
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\",\n",
        "        w_val = 1.\n",
        "    )\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsqwoVyLFvVB"
      },
      "source": [
        "How about when $w = 2$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g5K-_WsFvVF"
      },
      "outputs": [],
      "source": [
        "# w = 2.\n",
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\",\n",
        "        w_val = 2.\n",
        "    )\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmhRhr_MFvVF"
      },
      "outputs": [],
      "source": [
        "# w = 2.\n",
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\",\n",
        "        w_val = 2.\n",
        "    )\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN2EySOxFvVF"
      },
      "source": [
        "When $w$ is negative, the guidance is weak,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veEO4bN2FvVF"
      },
      "outputs": [],
      "source": [
        "# w = -1.\n",
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\",\n",
        "        w_val = -1.\n",
        "    )\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ke6LG06FvVF"
      },
      "outputs": [],
      "source": [
        "# w = -0.2\n",
        "generated = generate_new_images_cfg(\n",
        "        best_model,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        gif_name=\"mnist_cfg.gif\",\n",
        "        w_val = -0.2\n",
        "    )\n",
        "show_images_cfg(generated, n_classes, \"Classifier-free generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A83S6SiMFvVG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}